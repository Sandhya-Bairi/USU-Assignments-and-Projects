package com.usu.machinelearning.decisiontree.impurity;

import java.util.List;
import java.util.stream.Collectors;

import com.usu.machinelearning.decisiontree.data.DataSample;
import com.usu.machinelearning.decisiontree.label.Label;

/**
 * Entropy calculator. -p log2 p - (1 - p)log2(1 - p) - this is the expected information, in bits, conveyed by somebody
 * telling you the class of a randomly drawn example; the purer the set of examples, the more predictable this message
 * becomes and the smaller the expected information.
 * 
 * 
 *
 */
/**
 * @author Anuj Khasgiwala
 *
 */
public class EntropyCalculationMethod implements ImpurityCalculationMethod {

    @Override
    public double calculateImpurity(List<DataSample> splitData) {
        List<Label> labels = splitData.parallelStream().map(data -> data.getLabel()).distinct().collect(Collectors.toList());
        if (labels.size() > 1) {
            double p = getEmpiricalProbability(splitData, labels.get(0), labels.get(1)); // TODO fix to multiple labels
            return -1.0 * p * (Math.log(p)/Math.log(2)) - ((1.0 - p) * (Math.log(1.0 - p)/Math.log(2)));
        } else if (labels.size() == 1) {
            return 0.0; // if only one label data is pure
        } else {
            throw new IllegalStateException("This should never happen. Probably a bug.");
        }
    }
}