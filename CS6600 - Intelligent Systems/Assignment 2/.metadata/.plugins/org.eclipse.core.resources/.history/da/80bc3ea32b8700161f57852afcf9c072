package com.usu.machinelearning.decisiontree.impurity;

import java.util.List;
import java.util.stream.Collectors;

import com.usu.decisionTree.DecisionTree;
import com.usu.machinelearning.decisiontree.data.DataSample;
import com.usu.machinelearning.decisiontree.label.Label;

/**
 * Entropy calculator. -p log2 p - (1 - p)log2(1 - p) - this is the expected information, in bits, conveyed by somebody
 * telling you the class of a randomly drawn example; the purer the set of examples, the more predictable this message
 * becomes and the smaller the expected information.
 * 
 * @author Anuj Khasgiwala
 *
 */
public class EntropyCalculationMethod {

    public double calculateImpurity(DecisionTree<String> splitData) {
        List<Label> labels = splitData.parallelStream().map(data -> data.getLabel()).distinct().collect(Collectors.toList());
        if (labels.size() > 1) {
            double p = getEmpiricalProbability(splitData, labels.get(0), labels.get(1));
            return -1.0 * p * (Math.log(p)/Math.log(2)) - ((1.0 - p) * (Math.log(1.0 - p)/Math.log(2)));
        } else if (labels.size() == 1) {
            return 0.0; // if only one label data is pure
        } else {
            throw new IllegalStateException("This should never happen. Probably a bug.");
        }
    }
    
    default double getEmpiricalProbability(List<DataSample> splitData, Label positive, Label negative) {
        // TODO cache calculated counts
        return (double)splitData.parallelStream().filter(d -> d.getLabel().equals(positive)).count() / splitData.size();
    }
}